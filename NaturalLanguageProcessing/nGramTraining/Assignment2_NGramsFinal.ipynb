{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8o7mqUQkHR40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsTZR2LhM2fo",
        "outputId": "241ae63d-1042-4bd3-95c2-79d8ccb7bc93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter test text: Today we express our deepest gratitude to all those who have served in our armed forces.\n",
            "Enter n-gram size: 5\n",
            "+-------------------------------+------------+----------+-----------+-------+-----------+-------------+------+-------+---------+-------+--------+----------+----------+-------+---------+----------+-----------+\n",
            "|                               |      Today |       we |   express |   our |   deepest |   gratitude |   to |   all |   those |   who |   have |   served |       in |   our |   armed |   forces |         . |\n",
            "+===============================+============+==========+===========+=======+===========+=============+======+=======+=========+=======+========+==========+==========+=======+=========+==========+===========+\n",
            "| <s> <s> <s> <s>               | 0.00122034 | 0        |      0    |     0 |         0 |           0 |    0 |     0 |     0   |     0 |      0 |        0 | 0        |     0 |       0 |        0 | 0.0447458 |\n",
            "+-------------------------------+------------+----------+-----------+-------+-----------+-------------+------+-------+---------+-------+--------+----------+----------+-------+---------+----------+-----------+\n",
            "| <s> <s> <s> Today             | 0          | 0.444444 |      0    |     0 |         0 |           0 |    0 |     0 |     0   |     0 |      0 |        0 | 0.111111 |     0 |       0 |        0 | 0         |\n",
            "+-------------------------------+------------+----------+-----------+-------+-----------+-------------+------+-------+---------+-------+--------+----------+----------+-------+---------+----------+-----------+\n",
            "| <s> <s> Today we              | 0          | 0        |      0.25 |     0 |         0 |           0 |    0 |     0 |     0   |     0 |      0 |        0 | 0        |     0 |       0 |        0 | 0         |\n",
            "+-------------------------------+------------+----------+-----------+-------+-----------+-------------+------+-------+---------+-------+--------+----------+----------+-------+---------+----------+-----------+\n",
            "| <s> Today we express          | 0          | 0        |      0    |     1 |         0 |           0 |    0 |     0 |     0   |     0 |      0 |        0 | 0        |     1 |       0 |        0 | 0         |\n",
            "+-------------------------------+------------+----------+-----------+-------+-----------+-------------+------+-------+---------+-------+--------+----------+----------+-------+---------+----------+-----------+\n",
            "| Today we express our          | 0          | 0        |      0    |     0 |         1 |           0 |    0 |     0 |     0   |     0 |      0 |        0 | 0        |     0 |       0 |        0 | 0         |\n",
            "+-------------------------------+------------+----------+-----------+-------+-----------+-------------+------+-------+---------+-------+--------+----------+----------+-------+---------+----------+-----------+\n",
            "| we express our deepest        | 0          | 0        |      0    |     0 |         0 |           1 |    0 |     0 |     0   |     0 |      0 |        0 | 0        |     0 |       0 |        0 | 0         |\n",
            "+-------------------------------+------------+----------+-----------+-------+-----------+-------------+------+-------+---------+-------+--------+----------+----------+-------+---------+----------+-----------+\n",
            "| express our deepest gratitude | 0          | 0        |      0    |     0 |         0 |           0 |    1 |     0 |     0   |     0 |      0 |        0 | 0        |     0 |       0 |        0 | 0         |\n",
            "+-------------------------------+------------+----------+-----------+-------+-----------+-------------+------+-------+---------+-------+--------+----------+----------+-------+---------+----------+-----------+\n",
            "| our deepest gratitude to      | 0          | 0        |      0    |     0 |         0 |           0 |    0 |     1 |     0   |     0 |      0 |        0 | 0        |     0 |       0 |        0 | 0         |\n",
            "+-------------------------------+------------+----------+-----------+-------+-----------+-------------+------+-------+---------+-------+--------+----------+----------+-------+---------+----------+-----------+\n",
            "| deepest gratitude to all      | 0          | 0        |      0    |     0 |         0 |           0 |    0 |     0 |     0.5 |     0 |      0 |        0 | 0        |     0 |       0 |        0 | 0         |\n",
            "+-------------------------------+------------+----------+-----------+-------+-----------+-------------+------+-------+---------+-------+--------+----------+----------+-------+---------+----------+-----------+\n",
            "| gratitude to all those        | 0          | 0        |      0    |     0 |         0 |           0 |    0 |     0 |     0   |     1 |      0 |        0 | 0        |     0 |       0 |        0 | 0         |\n",
            "+-------------------------------+------------+----------+-----------+-------+-----------+-------------+------+-------+---------+-------+--------+----------+----------+-------+---------+----------+-----------+\n",
            "| to all those who              | 0          | 0        |      0    |     0 |         0 |           0 |    0 |     0 |     0   |     0 |      1 |        0 | 0        |     0 |       0 |        0 | 0         |\n",
            "+-------------------------------+------------+----------+-----------+-------+-----------+-------------+------+-------+---------+-------+--------+----------+----------+-------+---------+----------+-----------+\n",
            "| all those who have            | 0          | 0        |      0    |     0 |         0 |           0 |    0 |     0 |     0   |     0 |      0 |        1 | 0        |     0 |       0 |        0 | 0         |\n",
            "+-------------------------------+------------+----------+-----------+-------+-----------+-------------+------+-------+---------+-------+--------+----------+----------+-------+---------+----------+-----------+\n",
            "| those who have served         | 0          | 0        |      0    |     0 |         0 |           0 |    0 |     0 |     0   |     0 |      0 |        0 | 0.5      |     0 |       0 |        0 | 0         |\n",
            "+-------------------------------+------------+----------+-----------+-------+-----------+-------------+------+-------+---------+-------+--------+----------+----------+-------+---------+----------+-----------+\n",
            "| who have served in            | 0          | 0        |      0    |     1 |         0 |           0 |    0 |     0 |     0   |     0 |      0 |        0 | 0        |     1 |       0 |        0 | 0         |\n",
            "+-------------------------------+------------+----------+-----------+-------+-----------+-------------+------+-------+---------+-------+--------+----------+----------+-------+---------+----------+-----------+\n",
            "| have served in our            | 0          | 0        |      0    |     0 |         0 |           0 |    0 |     0 |     0   |     0 |      0 |        0 | 0        |     0 |       1 |        0 | 0         |\n",
            "+-------------------------------+------------+----------+-----------+-------+-----------+-------------+------+-------+---------+-------+--------+----------+----------+-------+---------+----------+-----------+\n",
            "| served in our armed           | 0          | 0        |      0    |     0 |         0 |           0 |    0 |     0 |     0   |     0 |      0 |        0 | 0        |     0 |       0 |        1 | 0         |\n",
            "+-------------------------------+------------+----------+-----------+-------+-----------+-------------+------+-------+---------+-------+--------+----------+----------+-------+---------+----------+-----------+\n",
            "| in our armed forces           | 0          | 0        |      0    |     0 |         0 |           0 |    0 |     0 |     0   |     0 |      0 |        0 | 0        |     0 |       0 |        0 | 1         |\n",
            "+-------------------------------+------------+----------+-----------+-------+-----------+-------------+------+-------+---------+-------+--------+----------+----------+-------+---------+----------+-----------+\n",
            "| our armed forces .            | 0          | 0        |      0    |     0 |         0 |           0 |    0 |     0 |     0   |     0 |      0 |        0 | 0        |     0 |       0 |        0 | 0         |\n",
            "+-------------------------------+------------+----------+-----------+-------+-----------+-------------+------+-------+---------+-------+--------+----------+----------+-------+---------+----------+-----------+\n",
            "| armed forces . </s>           | 0          | 0        |      0    |     0 |         0 |           0 |    0 |     0 |     0   |     0 |      0 |        0 | 0        |     0 |       0 |        0 | 0         |\n",
            "+-------------------------------+------------+----------+-----------+-------+-----------+-------------+------+-------+---------+-------+--------+----------+----------+-------+---------+----------+-----------+\n",
            "| forces . </s> </s>            | 0          | 0        |      0    |     0 |         0 |           0 |    0 |     0 |     0   |     0 |      0 |        0 | 0        |     0 |       0 |        0 | 0         |\n",
            "+-------------------------------+------------+----------+-----------+-------+-----------+-------------+------+-------+---------+-------+--------+----------+----------+-------+---------+----------+-----------+\n",
            "| . </s> </s> </s>              | 0          | 0        |      0    |     0 |         0 |           0 |    0 |     0 |     0   |     0 |      0 |        0 | 0        |     0 |       0 |        0 | 0         |\n",
            "+-------------------------------+------------+----------+-----------+-------+-----------+-------------+------+-------+---------+-------+--------+----------+----------+-------+---------+----------+-----------+\n",
            "| </s> </s> </s> </s>           | 0          | 0        |      0    |     0 |         0 |           0 |    0 |     0 |     0   |     0 |      0 |        0 | 0        |     0 |       0 |        0 | 0         |\n",
            "+-------------------------------+------------+----------+-----------+-------+-----------+-------------+------+-------+---------+-------+--------+----------+----------+-------+---------+----------+-----------+\n",
            "Perplexity for 'Today we express our deepest gratitude to all those who have served in our armed forces.': 43.79455280159716\n"
          ]
        }
      ],
      "source": [
        "#Tiara Devanathan 223025166\n",
        "#Prashantan Darshan Naidoo 223009965\n",
        "#Mahir Syed 223018507\n",
        "\n",
        "#References:\n",
        "#https://www.kaggle.com/code/alvations/n-gram-language-model-with-nltk\n",
        "#https://www.geeksforgeeks.org/n-gram-language-modelling-with-nltk/\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.lm import MLE\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline, pad_both_ends\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tabulate import tabulate\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def getCorpus(corpusFilePath, csvField = ''):\n",
        "  import pandas as pd\n",
        "  df = pd.read_csv(corpusFilePath)\n",
        "  return list(df[csvField].apply(word_tokenize))\n",
        "\n",
        "def train_model(corpus,n):\n",
        "  #Generate training data and vocabulary\n",
        "  training_data, vocab = padded_everygram_pipeline(n, corpus)\n",
        "\n",
        "  # Train MLE model\n",
        "  model = MLE(n)\n",
        "  model.fit(training_data, vocab)\n",
        "  return model\n",
        "\n",
        "def generate_ngram(text,n):\n",
        "  tokenized_text = word_tokenize(text)\n",
        "  return list(ngrams(pad_both_ends(tokenized_text,n),n-1))\n",
        "\n",
        "from tabulate import tabulate\n",
        "def probability_matrix(model,text,n):\n",
        "  # Row headers (given word)\n",
        "  given_words = generate_ngram(text, n)\n",
        "  # Column headers (next word)\n",
        "  next_words = word_tokenize(text)\n",
        "\n",
        "  table_data = []\n",
        "  # Add probabilities to the table data\n",
        "  for given in given_words:\n",
        "      row = [f\"{' '.join(given):<15}\"]  # Row header (given word)\n",
        "      for next in next_words:\n",
        "          prob = model.score(next, given)  # Probability\n",
        "          row.append(f\"{prob:.9f}\")  # Add the probability to the row\n",
        "      table_data.append(row)\n",
        "\n",
        "  # Add column headers\n",
        "  headers = [\" \"] + next_words\n",
        "\n",
        "  # Print the table\n",
        "  print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n",
        "\n",
        "\n",
        "def evaluate_perplexity(model, test_sentence, n):\n",
        "    test_data = list(ngrams(pad_both_ends(word_tokenize(test_sentence), n), n))\n",
        "\n",
        "    log_prob_sum = 0\n",
        "    word_count = 0\n",
        "\n",
        "    for ngram in test_data:\n",
        "        context, word = tuple(ngram[:-1]), ngram[-1]\n",
        "\n",
        "        prob = model.score(word, context)\n",
        "\n",
        "        if prob > 0:  # Avoid log(0) issues\n",
        "            log_prob_sum += np.log2(prob)\n",
        "            word_count += 1\n",
        "        else:\n",
        "            # Assign a small probability to avoid log(0)\n",
        "            log_prob_sum += np.log2(1e-10)\n",
        "            word_count += 1\n",
        "\n",
        "    perplexity = 2 ** (-log_prob_sum / word_count) if word_count > 0 else float('inf')\n",
        "\n",
        "    print(f\"Perplexity for '{test_sentence}': {perplexity}\")\n",
        "\n",
        "def main():\n",
        "    text = input(\"Enter test text: \")\n",
        "    n = int(input(\"Enter n-gram size: \"))\n",
        "\n",
        "    corpus = getCorpus('Donald-Tweets!.csv', 'Tweet_Text')\n",
        "\n",
        "    model = train_model(corpus, n)\n",
        "\n",
        "    probability_matrix(model, text, n)\n",
        "\n",
        "    evaluate_perplexity(model, text, n)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}